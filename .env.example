# LLM Configuration (Choose one)
ANTHROPIC_API_KEY=your_anthropic_key_here
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here

# Local LLM Support (Ollama, LM Studio, etc.)
LOCAL_LLM_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=llama3
GEMINI_MODEL=gemini-flash-latest

# Benchmark Configuration (Optional - defaults shown)
MAX_STEPS=20
MAX_RETRIES=3
RETRY_DELAY_MS=2000
DEFAULT_TOOL_TIMEOUT_MS=5000
CHROME_DEVTOOLS_TIMEOUT_MS=10000
CHROME_DEVTOOLS_SETTLE_DELAY_MS=1000
VIBIUM_WARMUP_DELAY_MS=2000
VERCEL_STABILIZATION_DELAY_MS=5000
# Set to 'false' for sequential execution (more stable, avoids race conditions)
PARALLEL_EXECUTION=true

# Target Application
TARGET_BASE_URL=http://localhost:3001
TARGET_PORT=3001

# Output Files
RESULTS_FILE=results.csv
REPORT_FILE=LAST_RUN_SUMMARY.md

# Logging (DEBUG, INFO, WARN, ERROR)
LOG_LEVEL=INFO
